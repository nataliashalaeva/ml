{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT5YqcUFp+xB0M3OYFvHvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliashalaeva/ml/blob/main/lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Выбор и загрузка набора данных. Загрузить датасет MNIST, содержащий\n",
        "изображения рукописных цифр, используя torchvision.datasets.MNIST. Преобразовать\n",
        "данные в формат тензоров и нормализуйте их с помощью torchvision.transforms.\n",
        "Организовать данные в пакеты (batch) для обучения, используя torch.utils.data.DataLoade"
      ],
      "metadata": {
        "id": "yXHye_YcpNrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y60CHzzoU_tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bac44bd-ef2b-49f1-cbe7-9178cd834eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.43MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 157kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.50MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.12MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Определение архитектуры модели. Создать класс модели нейронной сети на\n",
        "основе torch.nn.Module. Определить слои модели в конструкторе (__init__) и опишите\n",
        "прямой проход данных (forward) через слои"
      ],
      "metadata": {
        "id": "9ft-r2ijpgLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CWqVvxqgpgwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Выбор функции потерь и оптимизатора. Выберать функцию потерь (например,\n",
        "nn.CrossEntropyLoss) и оптимизатор (например, torch.optim.Adam) для обучения модели."
      ],
      "metadata": {
        "id": "ick26KlUpkGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNWYLQXIpotd",
        "outputId": "095f28b5-073e-4cbf-f94b-586c233dc7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Обучение модели. Написать цикл обучения, который перебирает пакеты данных,\n",
        "выполняет прямой проход, вычисляет потери, выполняет обратное распространение\n",
        "ошибки и обновляет параметры модели."
      ],
      "metadata": {
        "id": "osmantvuqPSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_epochs = 3\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 200 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], Loss: {running_loss / 200:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training finished in {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXs1G_F7qSY6",
        "outputId": "c6c66bd1-7862-4d4a-f738-9001d8e7ccd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step [200/938], Loss: 2.3131\n",
            "Epoch [1/3], Step [400/938], Loss: 2.3140\n",
            "Epoch [1/3], Step [600/938], Loss: 2.3132\n",
            "Epoch [1/3], Step [800/938], Loss: 2.3129\n",
            "Epoch [2/3], Step [200/938], Loss: 2.3143\n",
            "Epoch [2/3], Step [400/938], Loss: 2.3131\n",
            "Epoch [2/3], Step [600/938], Loss: 2.3131\n",
            "Epoch [2/3], Step [800/938], Loss: 2.3134\n",
            "Epoch [3/3], Step [200/938], Loss: 2.3133\n",
            "Epoch [3/3], Step [400/938], Loss: 2.3125\n",
            "Epoch [3/3], Step [600/938], Loss: 2.3138\n",
            "Epoch [3/3], Step [800/938], Loss: 2.3130\n",
            "Training finished in 45.76 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.0):\n",
        "        super(MyNeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "v9LMtykas2-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = MyNeuralNet(input_size, hidden_size, output_size, dropout_rate)\n",
        "input_data = torch.randn(1, input_size)\n",
        "output_data = model(input_data)\n",
        "print(\"Размер входных данных:\", input_data.shape)\n",
        "print(\"Размер выходных данных:\", output_data.shape)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhRs9ByV-aUN",
        "outputId": "c2e85428-ab1f-4290-f0c9-e014e0e47744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер входных данных: torch.Size([1, 10])\n",
            "Размер выходных данных: torch.Size([1, 5])\n",
            "MyNeuralNet(\n",
            "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
            "  (fc2): Linear(in_features=20, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Оценка производительности. Оценить точность (accuracy) модели на тестовом\n",
        "наборе данных."
      ],
      "metadata": {
        "id": "pkX2J7njrnwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Точность модели на тестовом наборе: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvVPKgjarqAL",
        "outputId": "6768f591-00d7-4aeb-def6-58112302f4de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность модели на тестовом наборе: 97.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Эксперименты с архитектурой и гиперпараметрами. Изменить архитектуру\n",
        "модели (например, добавьте больше сверточных слоев, увеличьте количество фильтров) и\n",
        "измените гиперпараметры обучения (например, скорость обучения, размер пакета), чтобы\n",
        "увидеть, как это повлияет на производительность. Провести несколько экспериментов и\n",
        "зафиксировать результаты.\n"
      ],
      "metadata": {
        "id": "He-dc2E1tYg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 3\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, learning_rate, batch_size, num_epochs, model_name):\n",
        "    print(f\"Training {model_name} with lr={learning_rate}, batch_size={batch_size}, epochs={num_epochs}\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "                running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "results = {}\n",
        "\n",
        "model1 = SimpleCNN()\n",
        "accuracy1 = train_and_evaluate(model1, learning_rate, batch_size, num_epochs, \"SimpleCNN Default\")\n",
        "results[\"SimpleCNN Default\"] = accuracy1\n",
        "\n",
        "model2 = SimpleCNN()\n",
        "accuracy2 = train_and_evaluate(model2, 0.0005, batch_size, num_epochs, \"SimpleCNN Smaller LR\")\n",
        "results[\"SimpleCNN Smaller LR\"] = accuracy2\n",
        "\n",
        "model3 = SimpleCNN()\n",
        "accuracy3 = train_and_evaluate(model3, learning_rate, 128, num_epochs, \"SimpleCNN Larger Batch\")\n",
        "results[\"SimpleCNN Larger Batch\"] = accuracy3\n",
        "\n",
        "model4 = DeeperCNN()\n",
        "accuracy4 = train_and_evaluate(model4, learning_rate, batch_size, num_epochs, \"DeeperCNN Default\")\n",
        "results[\"DeeperCNN Default\"] = accuracy4\n",
        "\n",
        "model5 = DeeperCNN()\n",
        "accuracy5 = train_and_evaluate(model5, 0.0005, batch_size, num_epochs, \"DeeperCNN Smaller LR\")\n",
        "results[\"DeeperCNN Smaller LR\"] = accuracy5\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for model_name, accuracy in results.items():\n",
        "    print(f\"{model_name}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsccPzl1ta5F",
        "outputId": "53fe390b-4744-40ad-94c4-76de6a531247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training SimpleCNN Default with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.542\n",
            "[1,   400] loss: 0.136\n",
            "[1,   600] loss: 0.089\n",
            "[1,   800] loss: 0.077\n",
            "[2,   200] loss: 0.061\n",
            "[2,   400] loss: 0.050\n",
            "[2,   600] loss: 0.053\n",
            "[2,   800] loss: 0.048\n",
            "[3,   200] loss: 0.035\n",
            "[3,   400] loss: 0.039\n",
            "[3,   600] loss: 0.040\n",
            "[3,   800] loss: 0.036\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 98.72%\n",
            "Training SimpleCNN Smaller LR with lr=0.0005, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.805\n",
            "[1,   400] loss: 0.192\n",
            "[1,   600] loss: 0.132\n",
            "[1,   800] loss: 0.105\n",
            "[2,   200] loss: 0.078\n",
            "[2,   400] loss: 0.075\n",
            "[2,   600] loss: 0.070\n",
            "[2,   800] loss: 0.064\n",
            "[3,   200] loss: 0.053\n",
            "[3,   400] loss: 0.049\n",
            "[3,   600] loss: 0.050\n",
            "[3,   800] loss: 0.052\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 98.46%\n",
            "Training SimpleCNN Larger Batch with lr=0.001, batch_size=128, epochs=3\n",
            "[1,   200] loss: 0.476\n",
            "[1,   400] loss: 0.130\n",
            "[1,   600] loss: 0.093\n",
            "[1,   800] loss: 0.072\n",
            "[2,   200] loss: 0.055\n",
            "[2,   400] loss: 0.048\n",
            "[2,   600] loss: 0.049\n",
            "[2,   800] loss: 0.046\n",
            "[3,   200] loss: 0.032\n",
            "[3,   400] loss: 0.037\n",
            "[3,   600] loss: 0.034\n",
            "[3,   800] loss: 0.036\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 98.76%\n",
            "Training DeeperCNN Default with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.472\n",
            "[1,   400] loss: 0.100\n",
            "[1,   600] loss: 0.076\n",
            "[1,   800] loss: 0.070\n",
            "[2,   200] loss: 0.045\n",
            "[2,   400] loss: 0.044\n",
            "[2,   600] loss: 0.042\n",
            "[2,   800] loss: 0.042\n",
            "[3,   200] loss: 0.028\n",
            "[3,   400] loss: 0.032\n",
            "[3,   600] loss: 0.030\n",
            "[3,   800] loss: 0.030\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 99.01%\n",
            "Training DeeperCNN Smaller LR with lr=0.0005, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.556\n",
            "[1,   400] loss: 0.115\n",
            "[1,   600] loss: 0.096\n",
            "[1,   800] loss: 0.066\n",
            "[2,   200] loss: 0.054\n",
            "[2,   400] loss: 0.049\n",
            "[2,   600] loss: 0.049\n",
            "[2,   800] loss: 0.045\n",
            "[3,   200] loss: 0.034\n",
            "[3,   400] loss: 0.039\n",
            "[3,   600] loss: 0.034\n",
            "[3,   800] loss: 0.032\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 99.17%\n",
            "\n",
            "Results:\n",
            "SimpleCNN Default: 98.72%\n",
            "SimpleCNN Smaller LR: 98.46%\n",
            "SimpleCNN Larger Batch: 98.76%\n",
            "DeeperCNN Default: 99.01%\n",
            "DeeperCNN Smaller LR: 99.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Эксперименты с функциями активации. Заменить функцию активации на\n",
        "другие и повторите обучение. Сравнить влияние различных функций активации на\n",
        "производительность."
      ],
      "metadata": {
        "id": "aUzM0ZFOw30-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, activation_fn):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.activation1 = activation_fn\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.activation2 = activation_fn\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.activation1(self.fc1(x))\n",
        "        x = self.activation2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VoVVNtCB1Ayv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activations = {\n",
        "    \"ReLU\": nn.ReLU(),\n",
        "    \"LeakyReLU\": nn.LeakyReLU(0.01),\n",
        "    \"ELU\": nn.ELU(),\n",
        "    \"Sigmoid\": nn.Sigmoid(),\n",
        "    \"Tanh\": nn.Tanh(),\n",
        "    \"Swish\": nn.SiLU(),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, activation in activations.items():\n",
        "    print(f\"\\nТест: {name}\")\n",
        "    model = SimpleNN(activation).to(device)\n",
        "    accuracy = train_and_evaluate(model, learning_rate=0.001, batch_size=64, num_epochs=3, model_name=name)\n",
        "    results[name] = accuracy\n",
        "\n",
        "print(\"\\nResults::\")\n",
        "for name, acc in results.items():\n",
        "    print(f\"{name}: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2eEzU1T1Ex-",
        "outputId": "14a18f3d-f935-42fb-9646-881164318f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Тест: ReLU\n",
            "Training ReLU with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.632\n",
            "[1,   400] loss: 0.356\n",
            "[1,   600] loss: 0.269\n",
            "[1,   800] loss: 0.226\n",
            "[2,   200] loss: 0.172\n",
            "[2,   400] loss: 0.160\n",
            "[2,   600] loss: 0.152\n",
            "[2,   800] loss: 0.147\n",
            "[3,   200] loss: 0.116\n",
            "[3,   400] loss: 0.115\n",
            "[3,   600] loss: 0.121\n",
            "[3,   800] loss: 0.110\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 95.92%\n",
            "\n",
            "Тест: LeakyReLU\n",
            "Training LeakyReLU with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.636\n",
            "[1,   400] loss: 0.331\n",
            "[1,   600] loss: 0.249\n",
            "[1,   800] loss: 0.214\n",
            "[2,   200] loss: 0.169\n",
            "[2,   400] loss: 0.143\n",
            "[2,   600] loss: 0.148\n",
            "[2,   800] loss: 0.141\n",
            "[3,   200] loss: 0.114\n",
            "[3,   400] loss: 0.105\n",
            "[3,   600] loss: 0.110\n",
            "[3,   800] loss: 0.107\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96.58%\n",
            "\n",
            "Тест: ELU\n",
            "Training ELU with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.554\n",
            "[1,   400] loss: 0.306\n",
            "[1,   600] loss: 0.244\n",
            "[1,   800] loss: 0.210\n",
            "[2,   200] loss: 0.155\n",
            "[2,   400] loss: 0.142\n",
            "[2,   600] loss: 0.133\n",
            "[2,   800] loss: 0.127\n",
            "[3,   200] loss: 0.102\n",
            "[3,   400] loss: 0.099\n",
            "[3,   600] loss: 0.107\n",
            "[3,   800] loss: 0.103\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96.20%\n",
            "\n",
            "Тест: Sigmoid\n",
            "Training Sigmoid with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 1.160\n",
            "[1,   400] loss: 0.409\n",
            "[1,   600] loss: 0.306\n",
            "[1,   800] loss: 0.263\n",
            "[2,   200] loss: 0.216\n",
            "[2,   400] loss: 0.189\n",
            "[2,   600] loss: 0.168\n",
            "[2,   800] loss: 0.166\n",
            "[3,   200] loss: 0.135\n",
            "[3,   400] loss: 0.129\n",
            "[3,   600] loss: 0.121\n",
            "[3,   800] loss: 0.115\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96.52%\n",
            "\n",
            "Тест: Tanh\n",
            "Training Tanh with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.564\n",
            "[1,   400] loss: 0.297\n",
            "[1,   600] loss: 0.234\n",
            "[1,   800] loss: 0.214\n",
            "[2,   200] loss: 0.158\n",
            "[2,   400] loss: 0.157\n",
            "[2,   600] loss: 0.153\n",
            "[2,   800] loss: 0.144\n",
            "[3,   200] loss: 0.128\n",
            "[3,   400] loss: 0.128\n",
            "[3,   600] loss: 0.124\n",
            "[3,   800] loss: 0.127\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96.84%\n",
            "\n",
            "Тест: Swish\n",
            "Training Swish with lr=0.001, batch_size=64, epochs=3\n",
            "[1,   200] loss: 0.576\n",
            "[1,   400] loss: 0.284\n",
            "[1,   600] loss: 0.218\n",
            "[1,   800] loss: 0.191\n",
            "[2,   200] loss: 0.143\n",
            "[2,   400] loss: 0.132\n",
            "[2,   600] loss: 0.130\n",
            "[2,   800] loss: 0.115\n",
            "[3,   200] loss: 0.100\n",
            "[3,   400] loss: 0.091\n",
            "[3,   600] loss: 0.093\n",
            "[3,   800] loss: 0.098\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 97.25%\n",
            "\n",
            "Results::\n",
            "ReLU: 95.92%\n",
            "LeakyReLU: 96.58%\n",
            "ELU: 96.20%\n",
            "Sigmoid: 96.52%\n",
            "Tanh: 96.84%\n",
            "Swish: 97.25%\n"
          ]
        }
      ]
    }
  ]
}